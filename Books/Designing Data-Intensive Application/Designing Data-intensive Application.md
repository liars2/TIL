큐나 데이터베이스마다 특징이 있는데 이들의 데이터 접근 방식은 매우 다른 패턴을 가지고 있어 성능 특성이 있기 때문에 구현 방식이 전혀 다르다

그럼에도 불구하고 데이터 시스템이라는 포괄적인 언어로 이들을 범주화 하였는데 이유는 다음과 같다.

1. 기존 데이터베이스와 **일시적으로 저장하는** 큐 형태의 Redis 등 여러가지 데이터 저장을 도와주는 솔루션이 존재하는데 일시적으로 저장하지 않고 지속적인 큐 저장소인 Kafka의 등장으로 경계가 흐려지고 있다.

2. 데이터 베이스 하나만 존재하는 것이 아니라 여러가지의 역할들로 이루어져있다. (작업은 단일 도구에서 효율적으로 수행할 수 있는 태스크로 나누고 애플리케이션 코드로 인해 수행한다.)

서비스 제공을 위해 각 도구를 결합할 때 서비스 인터페이스나 API는 클라이언트가 모르게 구현 세부 사항을 숨긴다.

기본적으로 좀 더 작은 범용 구성 요소들로 새롭고 특수한 목적의 데이터 시스템을 만든다.

그래서 개발자는 애플리케이션 개발자 뿐만 아니라 데이터 시스템 설계자이기도 한다.

데이터 시스템 혹은 서비스를 설계 하려고 할 때 까다로운 문제가 많이 생긴다.

ex) 내부 문제에도 불구하고 데이터 정확하여 일관성 유지? 일부 성능이 저하되더라도 클라이언트에 일관되거 좋은 성능 제공? 부하 증가를 위해 규모 확장? 서비스를 위해 좋은 API?

그래서 데이터-중심 어플리케이션 시스템에서는 크게 3가지를 다룬다

# 1. 신뢰성

## 신뢰성을 왜 보장?
[컴퓨팅 뿐만 아니라, 신뢰성은 굉장히 중요하다](https://tasminrl.wordpress.com/2011/09/30/why-is-reliability-important/)
    누구나 어떤 것을 신뢰하거나 신뢰하지 않는다는 의미가 무엇인지에 대한 직관적인 개념을 가지고 있다.
    소프트웨어의 경우 일반적인 기대치는 다음과 같다.

- 어플리케이션은 사용자가 기대한 기능을 수행한다.
- 시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족한다.

- 시스템은 사용자가 범한 실수나 예상치 못한 소프트웨어 사용법을 허용할 수 있다.
- 시스템은 허가되지 않은 접근과 오남용을 방지한다.

양간에 의미가 서로 침해하는 부분이 있는데 이에 관해서 설명을 더 하자면
시스템은 유저가 사용하는 소프트웨어의 예외를 인정하고 허용하되, 허가되지 않는 접근과 남용을 예방해야한다.

### 장애와 결함

    일반적으로 결함은 사양에서 벗어난 시스템의 한 구성 요소로 정의되지만, 장애는 사용자에게 필요한 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우다

결함 같은 경우 크게는 총 3 가지로 분리할 수 있다.

### 하드웨어

    물리적인 상황에서 벌어질 수 있는 상황을 말한다
    ex) 하드디스크 고장, 정전, 서버 내부의 네트워크 장애

시스템 장애율을 줄이기 위해서의 대응은 하드웨어 구성 요소에 중복을 추가하는 방법이 일반적이다.

[디스크는 RAID 구성으로 설치할 수 있고](https://www.wdc.com/ko-kr/solutions/raid.html) 서버는 [이중 전원 디바이스와](https://www.quora.com/What-is-a-dual-power-supply-in-electronics) [핫 스왑](https://ko.wikipedia.org/wiki/%ED%95%AB_%EC%8A%A4%EC%99%80%ED%95%91) 가능한 CPU를, 데이터 센터는 건전지와 예비 전원용 디젤 발전기를 갖출 수 있다.

기존까지 단일 장비의 전체 장애는 매우 드물기 때문에 대부분의 애플리케이션은 하드웨어 구성요소의 중복으로 충분하였다(새 장비에 백업을 빠르게 하여 다운타임은 치명적이지 않았기 때문)

하지만 데이터 양과 애플리케이션의 계산 요구가 늘어나면서 더 많은 애플리케이션이 많은 수의 장비를 사용하게 되었고 하드웨어 결함율도 증가하게 되었다.

AWS 같은 일부 클라우드 플랫폼은 가상 장비 인스턴스가 별도의 경고없이 사용할 수 없게 되는 상황이 상당히 일반적이다.

이런 플랫폼은 단일 장비 신뢰성보다 유연성과 탄력성을 우선적으로 처리하게끔 설계됐기 때문이다.

## 소프트웨어

하드웨어 결함은 상관관계가 없어서 동시에 장애가 발생하지는 않는다. (약한 상관관계 제외)

하지만 체계적 오류 때문에 시스템 오류를 더욱 많이 유발하는데

이 결함은 하드웨어 결함보다 시스템 오류를 더욱 많이 유발하는 경향이 있다.

### 시스템 내 체계적 오류

- [잘못된 특정 입력이 있을 때](https://www.enersystems.com/2016/08/02/this-years-leap-second-could-cause-some-computers-to-crash/) 모든 애플리케이션 서버 인스턴스가 죽는 소프트웨어 버그
- CPU 시간, Memory, Disk Space, Network bandwidth처럼 공유 자원을 과도하게 사용하는 일부 프로세스
- 시스템의 속도가 느려져 반응이 없거나 잘못된 응답을 반환하는 서비스
- 한 구성 요소의 작은 결함이 다른 구성 요소의 결함을 야기하고 차례차례 더 많은 결함이 발생하는 연쇄장애

**소프트웨어 결함을 유발하는 버그는 특정 상홍에 의해 발생하기 전까지 오랫동안 나타나지 않는다.**

소프트웨어의 체계적 오류 문제는 신속한 해결책이 없다. 그래서 이와 같은 주의를 해야한다.

- 시스템의 가정과 상호작용에 대해 주의 깊게 생각하기
- 빈틈없는 테스트
- [프로세스 격리](https://en.wikipedia.org/wiki/Process_isolation)
- 죽은 프로세스의 재시작 허용
- 프로덕션 환경에서 시스템 동작의 측정
- 모니터링및 분석하기

시스템이 뭔가를 보장하길 기대한다면 소행 중에 이를 지속적으로 확인해 차이가 생기는 경우 경고를 발생하여 문제 해결에 도움을 줄 수 있다.

### 인적오류

사람은 소프트웨어 시스템을 설계하고 구축하며, 운영자로서 시스템을 계속 운영한다.

사람이 아무리 최선을 다해도 실수를 하기는 마련인데 어느정도냐면 대규모 인터넷 서비스에 대한 대부분의 실패율은 관리자의 잘못된 설정 오류가 주요 원인이지만 그럼에도 불구하고 시스템을 어떻게 신뢰성 있게 만들까?

- 오류의 가능성을 최소화하는 방향으로 시스템을 설계하라. ex) 잘 설계된 추상화, API, 관리 인터페이스..
- 사람이 가장 많이 실수하는 장소에서 사람의 실수로 장애가 발생할 수 있는 부분을 분리하라, 실제 데이터를 사용해 안전하게 살펴보고 실험할 수 있지만 실제 사용자에게는 영향이 없는 비 프로덕션 샌드박스를 제공하라.
- 단위 테스트로부터 전체 시스템 통합 테스트와 수동 테스트까지 모든 수준에서 철저하게 테스트하라
- 장애 발생의 영향을 최소화하기 위해 인전 오류를 빠르고 쉽게 복구할 수 있게 하라.
- 성능 지표와 오류율 같은 상세하고 명확한 모니터링 대책을 마련하라.

신뢰성은 얼마나 중요할까

원자력 발전소나 항공 교통 관제 소프트웨어 뿐만 아니라 더 많은 수의 일상적인 비지니스 어플리케이션에서도 버그는 생산성 저하의 원인이고,
전자 상거래 사이트의 중단은 매출에 손실이 발생하고 명성에 타격을 준다는 면에서 많은 비용이 든다.

증명되지 않은 시장을 위해 프로토타입을 개발하는 비용이나 매우 작은 이익율의 서비스를 운영하는 비용을 줄이려 신뢰성을 희생해야 하는 상황이
이 경우에는 비용을 줄여야 하는 시점을 매우 잘 알고 있어야 한다.

# 2. 확장성

현재 시스템이 안정적으로 동작한다고 해서 미래에도 안정적으로 동작한다는 보장은 없다. 대부분의 성능저하를 유발하는 흔한 이유 중 하나는 부하 증가다.
"시스템이 특정 방식으로 커지면 이에 대처하기 위한 선택은 무엇인가?" 혹은 "추가 부하를 다루기 위해 계산 자원을 어떻게 투입할까?" 같은 질문들은 다 확장성과 관련이 있다.

## 부하 기술하기

    확장성에 대하여 논의하기 전에 부하를 먼저 간결하게 기술해야한다.
    그래야 부하 성장 질문을 논의할 수 있다.


부하는 부하 매개변수라 부르는 몇 개의 숫자로 나타낼 수 있다. 가장 적합한 부하 매개변수 선택은 시스템 설계에 따라 달라진다.
부하 매개변수로 웹 서버의 초당 요청수, 데이터베이스의 읽기 대 쓰기 비율, 대화방의 동시 활성 사용자, 캐시 적중률 등이 될 수 있다.
평균적인 경우가 중요할 수도 있고 소수의 극단적인 경우가 병목 현상의 원인일 수도 있다.

## 성능 기술하기

    부하를 기술하면 부하가 증가할 떄 어떤일이 일어날지는 다음 두가지 방법으로 조사할 수 있다.

- 부하 매개변수를 증가시키고 시스템 자원(CPU, 메모리, 네트워크 대역폭 등)은 변경하지 않고 유지하면 시스템 성능은 어떻게 영향을 받을까?
- 부하 매개변수를 증가시켰을 때 성능이 변하지 않고 유지되길 원한다면 자원을 얼마나 많이 늘려야 할까?

두 질문 모두 성능 수치가 필요하다. 따라서 시스템 성능에 간단히 살펴보자면

하둡 같은 일괄 처리 시스템은 보통 **처리량에** 관심을 가지고 온라인 시스템에서 더 중요한 사항은 서비스 **응답시간**, 즉 클라이언트가 요청을 보내고 응답을 받는 사이의 시간이다.

클라이언트가 몇 번이고 반복해서 동일한 요청을 하더라도 매번 응답 시간이 다르다.

**실제로 다양한 요청을 다루는 시스템에서 응답 시간은 많이 변하므로 응답 시간은 단일 숫자가 아니라 측정 가능한 값의 분포로 생각해야 한다.**

분표를 지표로 만들어서 본다면 대부분의 요청은 빠르지만 가끔 꽤 오래 걸리는 특이 값이 존재하는데, 해당 요청은 더 많은 데이터를 처리하기 때문에 본질적으로 더 비쌀진도 모른다.

하지만 모든 요청에 동일한 시간이 걸려야 한다는 상황에서도 Garbage Collection puase, page fault, 서버 랙의 기계적인 진동이나 다른 원인으로 추가 지연이 생길 수 있다.

서비스 평균 응답 시간을 살피는 것 보다 "전형적인" 응답 시간을 알고 싶다면 백분위를 사용하는 편이 좋다.

## 중앙값

사용자가 보통 얼마나 오랫동안 기다려야 하는지 알고 싶다면 중앙값이 좋은 지표다.

사용자 요청의 절반은 중앙값 응답 시간 미만으로 제공되고, 나머지 반은 중앙값보다 오래 걸린다.

중앙값은 50분위로서 50p로 축약할 수 있다. 중앙값은 단일 요청을 참고한다는 점을 주의하자.

사용자가 여러개의 요청을 보내면 최소한 하나의 요청이 중앙값보다 느릴 확률이 50%보다 훨씬 높다.

특이 값이 얼마나 좋지 않은지 알아보려면 상위 백분위를 살펴보는 것도 좋다.

이때 사용하는 백분위는 95, 99, 99.9 분위가 일반적이다. 요청의 앞에 나온 분위보다 빠르다면 해당 특정 기준치가 각 백분위의 응답 시간 기준치가 된다.

### 꼬리 지연 시간

    꼬리 지연 시간으로 알려진 상위 백분위 응답 시간은 서비스의 사용자 경험에 직접 영향을 주기 때문에 중요하다.

아마존은 내부 서비스의 응답 시간 요구샇랑을 99.9분위로 기술한다 보통 응답 시간이 **가장 느린 요청을 경험한 고객들은 많은 구매를 해서 고객 중에서 계정에 가장 많은 데이터를 갖고 있어서다.**

웹 사이트를 빠르게 제공 가능하게끔 보장해 고객을 계속 행복하게 만드는게 중요하다.

아마존은 응답 시간이 100밀리초 증가하면 판매량이 1% 줄어들고 1초가 느려지면 고객의 만족도 지표는 16% 줄어드는 현상을 관찰했다.

반면 99.99분위를 최적화하는 작업에는 비용이 너무 많이 들어서 아마존이 추구하는 목표에 충분히 이익을 가져다주지는 못한다고 여겨진다.

### 큐 대기 지연

    높은 백분위에서 응답 시간의 상당 부분을 차지한다.
    서버는 병렬로 소수의 작업만 처리할 수 있기 때문에 소수의 느린 요청 처리만으로도 후속 요청 처리가 지체된다.

무슨소리냐면, 편의점 물품 구입으로도 예를 들 수 있다. 편의점 물품을 계산하려고 줄을 섰는데 앞에 사람이 엄청 많은 물건을 계산하고 있으면 무슨 생각이 드는가?

나의 물품은 10초 안에 계산하고 끝날 수 있는데 앞에 있는 지연 때문에 덩달아 내 시간도 뺏기고 있다는 소리다.

이 현상을 **선두 차단이라고 한다.** 서버에서 후속 요청이 빠르게 처리되더라도 이전 요청이 완료되길 기다리는 시간 때문에 클라이언트는 전체적으로 응답 시간이 느리다고 생각할 것이다.

시스템의 확장성을 테스트하려고 인위적으로 부하를 생성하는 경우 부하 생성 클라이언트는 응답시간과 독립적으로 요청을 지속적으로 보내야 한다.

만약 클라이언트가 다음 요청을 보내기 전에 이전요청이 완료되길 기다리면 테스트에서 인위적으로 대기 시간을 실제보다 더 짧게 만들어 평가를 왜곡한다.

이런 문제 때문에 클라이언트 쪽 응답 시간 측정이 중요하다.

## 부하 대응

    성능 측정을 위한 부하와 지표를 기술하는 매개변수에 대해 설명했으니 본격적으로 확장성 논의를 시작한다. 부하 매개변수가 어느 정도 증가하더라도 좋은 성을을 유지하려면 어떻게 해야 할까?
    버틸 수 있는 부하 수준에서 10배 정도 되는 부하를 받으면 대응할 수 없다. 급성장하는 서비르르 맡고 있다면 부하 규모의 자릿수가 바뀔 때마다 혹은 그보다 자주 아키텍처를 재검토해야 할지 모른다.

그래서 사람들은 확장성과 관련해 더 강력한 장비로 이동하는 용량 확장(Scaling up), 그리고 다수의 낮은 장비를 사용하는 규모 확장(Scale out)으로 구분해서 말하곤 한다.

다수의 장비에 부하를 분산하는 아키텍처를 비공유(shared-nothing) 아키텍처라 부른다.

단일 장비에서 수행될 수 있는 시스템은 보통 간단하지만 고사양 장비는 매우 비싸기 때문에 상당히 집약된 작업 부하는 대개 규모 확장을 피하지 못한다.

현실적으로 좋은 아키텍처는 실용적인 접근 방식의 조합이 필요하다. (적절한 사양의 장비 몇 대가 다랑의 낮은 사양 가상 장비보다 훨씬 간단하고 저렴하다)

일부 시스템은 부하 증가를 감지하면 컴퓨팅 자원을 자동으로 추가할 수 있는데, 그렇지 않은 시스템은 수동으로(사람이 직접 확인 후 대응) 확장해야 한다.

**탄력적인 시스템은 부하를 예측할 수 없을 만큼 높은 경우 유용하지만 수동으로 확장하는 시스템이 더 간단하고 운영상 예상치 못한 일이 더 적다.**

다수의 장비에 stateless 서비스를 배포하는 일은 상당히 간단하지만, 단일 노드에 stateful 데이터 시스템을 분산 설치하는 일은 아주 많은 복잡도가 추가적으로 발생한다.

이런 이유로 확장 비용이나 데이터베이스를 분산으로 만들어야 하는 고가용성 요구가 있을 때 까지 단일 노드에 데이터베이스를 유지하는 것이 최근까지의 통념이다.

대개 대규모로 동작하는 시스템의 아키텍처는 사용하는 애플리케이션에 특화돼어 있으며.
> 이런 아키텍처를 결정하는 요소는 읽기의 양, 쓰기의 양, 저장할 데이터의 양, 데이터의 복잡도, 응답 시간 요구사항, 접근 패턴 등이 있다.

# 3. 유지보수성

[소프트웨어 비용은 대부분은 초기 개발이 아니라 지속해서 이어지는 유지보수에 들어간다는 사실은 잘 알려져있다.](https://www.quora.com/What-is-harder-maintaining-code-or-creating-new-code)

이런 유지보수에는 버그 수정, 시스템 운영 유지, 장애 조사, 새로운 플랫폼 적응, 새 사용 사례를 위한 변경, 기술 채무 상환, 새로운 기능 추가 등이 존재한다.

소프트웨어 시스템상에서 일하는 사람들은 레거시 시스템 유지보수 작업을 좋아하지 않는다.

다른 사람의 실수를 고쳐야 하거나 한물 간 플랫폼에서 작업해야 하거나 정말 하기 싫은 일을 해야 하는 시스템에 관여해야 하기 때문이다.

모든 레거시 시스템은 각자 나름대로의 불편함이 있다, 그래서 이 레거시 코드를 만들지 않도록 최소화할 수 있는 소프트웨어 시스템 설계 원칙은 다음 세 가지다.

- 운용성
    - 운영팀이 시스템을 원활하게 운영할 수 있도록 쉽게 만들어라.

- 단순성
    - 시스템에서 복잡도를 최대한 제거해 새로운 엔지니어가 시스템을 이해하기 쉽게 만들어라 (인터페이스의 단순성과는 다르다)

- 발전성
    - 엔지니어가 이후에 시스템을 쉽게 변경할 수 있게 하라. 그래야 요구사항 변경 같은 예기치 않은 사용 사례를 적용하기가 쉽다.
    - 이 속성은 유연성, 수정 가능성, 적응성으로 알려져 있다.


신뢰성, 확장성을 달성하기 위한 쉬운 해결책은 없다. 그보다 운용성 단순성, 발전성을 염두에 두고 시스템을 생각하려 노력해야 한다.

## 운용성의 극대화

    "좋은 운영은 종종 불완전한 소프트웨어의 제약을 피하는 대안이 될 수 있다. 하지만 좋은 소프트웨어라도 나쁘게 운영할 경우 작동을 신뢰할 수 없다."

운영 중 일부 측면은 자동화할 수 있고 또 자동화해야 한다. 그러나 자동화를 처음 설정하고 제대로 동작하는지 확인하는 일은 여전히 사람의 몫이다.

## 운영팀이 책임을 져야할 항목

- 시스템 상태 모니터링, 장애 복구
- 시스템 장애, 성능 저하, etc 등의 문제 원인 추적
- 보안 패치를 포함해 소프트웨어와 플랫폼을 최신 상태로 유지
- 다른 시스템이 서로 어떻게 영향을 주는지 확인해 문제가 생길 수 있는 변경 사항을 손상을 입히기 전에 차단
- 배포, 설정 관리 등을 위한 모범 사례와 도구를 마련
- 애플리케이션 특정 플랫폼에서 다른 플랫폼으로 이동하는 등 복잡한 유지보수 태스크를 수행
- 설정 변경으로 생기는 시스템 보안 유지보수
- 예측 가능한 운영과 안정적인 서비스 환경을 유지하기 위한 절차 정의
- 개인 인사 이동에도 시스템에 대한 조직의 지식을 보존함

좋은 운영성이란 동일하게 반복되는 태스크를 쉽게 수행하게끔 만들어 운영팀이 고부가가치 활동에 노력을 집중한다는 의미다.
- 반복작업 쉽게 만들어 운영팀이 다른 고비용 활동 집중

데이터 시스템은 동일 반복 태스크를 쉽게 하기 위해 아래 항목 등을 포함해 다양한 일을 할 수 있다.

- 좋은 모니터링으로 런타임 동작과 시스템의 내부에 대한 가시성 제공
- 표준 도구를 이용해 자동화와 통합을 위한 우수한 지원을 제공
- 개별 장비 의존성 회피. 유지보수를 위해 장비를 내리더라도 시스템 전체에 영향을 주지 않고 계속해서 운영 가능해야 함
- 좋은 문서와 이해하기 쉬운 운영모델 제공
- 만족할 만한 기본 동작을 제공하고, 필요할 때 기본값을 다시 정의할 수 있는 자유를 관리자에게 부여
- 적절하게 자기회복이 가능할 뿐 아니라 필요에 따라 관리자가 시스템 상태를 수동으로 제어 가능하게 함
- 예측 가능히게 동작하고 예기치 않은 상황을 최소화 함

## 복잡성관리

    소규모 소프트웨어 프로젝트에서는 간단하고 표현이 풍부한 코드로 말끔히 작성하지만, 프로젝트가 커짐에 따라 시스템은 매우 복잡하고 이해하기 어려워진다.
    복잡도 같은 시스템에서 작업해야 하는 모든 사람의 진행을 느리게 하고 나아가 유지보수 비용이 증가한다.

복잡도 수렁에 빠진 소프트웨어 프로젝트를 때론 커다란 진흙 덩어리로(스파게티 코드) 묘사한다.

복잡도는 다양한 증상으로 나타난다.

- 상태 공간의 급증
- [모듈 간 강한 커플링](https://www.webopedia.com/TERM/T/tight_coupling.html)
- 일관성 없는 명명과 용어
- 성능 문제 해결을 목표로 한 해킹
- 임시방편으로 문제를 해결한 특수 사례 등..

복잡도 때문에 시스템 유지보수가 어려울 때 에산과 일정이 초과되곤 한다. 복잡한 소프트웨어에서는 변경이 있을 때 버그가 생길 위험이 더 크다.

개발자가 시스템을 이해하고 추론하기 어려워지면 시스템에 숨겨진 가정과 의도치 않은 결과 및 예기치 않은 상호작용을 간과하기 쉽다.

반대로 복잡도를 줄이면 소프트웨어 유지보수성이 크게 향상되므로, 단순성이 구축하려는 시스템의 핵심 목표여야 한다.

시스템을 단순하게 만드는 일이 반드시 기능을 줄인다는 의미는 아니다.

우발적 복잡도를 줄인다는 뜻일 수도 있다.

### 우발적 복잡도

    소프트웨어가 풀어야 할 문제에 내재하고 있지 않고 구현에서만 발생하는 것

우발적 복잡도를 제거하기 위한 최상의 도구는 추상화다. 좋은 추상화는 깔끔하고 직관적인 외과 아래로 많은 세부 구현을 숨길 수 있다. 또한 좋은 추상화는 다른 다양한 애플리케이션에도 사용가능하다.

이러한 재사용은 비슷한 기능 여러 번 재구현하는 것보다 더 효율적일 뿐만 아니라 고품질 소프트웨어로 이어진다.

추상화된 구성 요소의 품질 향상이 이를 사용하는 모든 애플리케이션에 도움이 되기 때문이다.

# 변화를 쉽게 만들기

    시스템의 요구사항이 영원히 바뀌지 않을 가능성은 매우 적다.
    그러므로 항상 바뀌는 요구사항에 대비를 해야한다.

조직 프로세스 측면에서 애자일 작업 패턴은 변화에 적응하기 위한 프레임워크를 제공한다.

애자일 관련 문서는 [익스트림 프로그래밍](https://ko.wikipedia.org/wiki/%EC%9D%B5%EC%8A%A4%ED%8A%B8%EB%A6%BC_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D) 관련 문서를 참조하길 바란다.



